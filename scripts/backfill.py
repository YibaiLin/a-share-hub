"""
å†å²æ•°æ®å›å¡«è„šæœ¬

ç”¨äºæ‰¹é‡é‡‡é›†å…¨å¸‚åœºå†å²æ—¥çº¿æ•°æ®
"""

import sys
import asyncio
import argparse
from pathlib import Path
from datetime import datetime
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.logger import setup_logger, logger
from core.database import get_db_client, close_db_client
from collectors.stock_list import StockListCollector
from collectors.daily import DailyCollector
from storage.clickhouse_handler import ClickHouseHandler
from utils.progress import ProgressTracker
from utils.date_helper import format_date
from utils.failure_monitor import FailureMonitor
from utils.rate_limit_detector import RateLimitDetector, is_rate_limit_error


async def backfill_all_stocks(
    start_date: str,
    end_date: str,
    concurrency: int = 1,
    resume: bool = False
):
    """
    å›å¡«å…¨å¸‚åœºè‚¡ç¥¨æ•°æ®

    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDï¼‰
        concurrency: å¹¶å‘æ•°ï¼ˆé»˜è®¤1ï¼‰
        resume: æ˜¯å¦æ–­ç‚¹ç»­ä¼ 

    Returns:
        é‡‡é›†æŠ¥å‘Šå­—å…¸
    """
    logger.info("=" * 80)
    logger.info("å¼€å§‹å†å²æ•°æ®å›å¡«")
    logger.info("=" * 80)
    logger.info(f"æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    logger.info(f"å¹¶å‘æ•°: {concurrency}")
    logger.info(f"æ–­ç‚¹ç»­ä¼ : {'æ˜¯' if resume else 'å¦'}")

    start_time = datetime.now()

    try:
        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
        tracker = ProgressTracker()

        # è·å–è‚¡ç¥¨åˆ—è¡¨
        logger.info("è·å–è‚¡ç¥¨åˆ—è¡¨...")
        stock_list_collector = StockListCollector()
        all_stocks = await stock_list_collector.get_all_stocks()
        logger.info(f"è·å–åˆ° {len(all_stocks)} åªè‚¡ç¥¨")

        # æ£€æŸ¥æ˜¯å¦æ¢å¤è¿›åº¦
        if resume and tracker.has_progress():
            logger.info("æ£€æµ‹åˆ°å·²æœ‰è¿›åº¦ï¼Œç»§ç»­å›å¡«")
            stocks_to_collect = tracker.get_remaining_stocks(all_stocks)
            tracker.print_summary()
        else:
            logger.info("å¼€å§‹æ–°çš„å›å¡«ä»»åŠ¡")
            tracker.init_progress(start_date, end_date, len(all_stocks))
            stocks_to_collect = all_stocks

        if not stocks_to_collect:
            logger.info("æ‰€æœ‰è‚¡ç¥¨å·²å®Œæˆï¼Œæ— éœ€å›å¡«")
            return _generate_report(tracker, start_time)

        logger.info(f"å¾…é‡‡é›†: {len(stocks_to_collect)} åªè‚¡ç¥¨")

        # åˆå§‹åŒ–é‡‡é›†å™¨å’Œå­˜å‚¨
        daily_collector = DailyCollector()
        client = get_db_client()
        handler = ClickHouseHandler(client)

        # åˆå§‹åŒ–å¤±è´¥ç›‘æ§å™¨
        failure_monitor = FailureMonitor(
            threshold=10,  # è¿ç»­å¤±è´¥10æ¬¡è§¦å‘æš‚åœ
            pause_duration=60,  # æš‚åœ60ç§’
            enable=True
        )
        logger.info(f"å¤±è´¥ç›‘æ§: é˜ˆå€¼={failure_monitor.threshold}, æš‚åœæ—¶é•¿={failure_monitor.pause_duration}ç§’")

        # åˆå§‹åŒ–æ™ºèƒ½é™æµæ¢æµ‹å™¨ï¼ˆæŒ‡å®šæ•°æ®æºå’Œæ¥å£ï¼‰
        rate_limit_detector = RateLimitDetector(
            enable=True,
            source="akshare",
            interface="stock_zh_a_hist",
            data_type="daily",
            description="Aè‚¡æ—¥çº¿æ•°æ®"
        )
        logger.info("æ™ºèƒ½é™æµæ¢æµ‹: å·²å¯ç”¨")

        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(concurrency)

        # æ‰¹é‡é‡‡é›†
        success_count = 0
        failed_count = 0

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(
            total=len(stocks_to_collect),
            desc="é‡‡é›†è¿›åº¦",
            unit="åª",
            ncols=100
        ) as pbar:
            for ts_code in stocks_to_collect:
                async with semaphore:
                    # æ£€æŸ¥å¤±è´¥ç›‘æ§å™¨æ˜¯å¦éœ€è¦æš‚åœ
                    failure_monitor.wait_if_paused()

                    # æ£€æŸ¥æ™ºèƒ½é™æµæ¢æµ‹å™¨æ˜¯å¦éœ€è¦æš‚åœ
                    should_wait, wait_seconds = await rate_limit_detector.should_pause()
                    if should_wait:
                        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºç­‰å¾…çŠ¶æ€
                        if rate_limit_detector.state == "PROBING":
                            pbar.set_postfix_str(f"æ¢æµ‹ç­‰å¾… (ç¬¬{rate_limit_detector.probe_count + 1}æ¬¡)")
                        elif rate_limit_detector.state == "CONFIRMED":
                            pbar.set_postfix_str(f"æ™ºèƒ½æš‚åœ ({wait_seconds}ç§’)")
                        await asyncio.sleep(wait_seconds)

                    try:
                        # æ›´æ–°è¿›åº¦æ¡æè¿°
                        pbar.set_postfix_str(f"å½“å‰: {ts_code}")

                        # é‡‡é›†æ•°æ®
                        data = await daily_collector.collect(
                            symbol=ts_code,
                            start_date=start_date,
                            end_date=end_date
                        )

                        # è®°å½•æˆåŠŸè¯·æ±‚åˆ°é™æµæ¢æµ‹å™¨
                        rate_limit_detector.record_success()

                        # å¦‚æœåœ¨æ¢æµ‹é˜¶æ®µè¯•æ¢æˆåŠŸï¼Œç¡®è®¤çª—å£
                        if rate_limit_detector.state == "PROBING":
                            await rate_limit_detector.on_probe_success()
                            logger.info("ç»§ç»­é‡‡é›†...")

                        if not data:
                            logger.debug(f"{ts_code} æ— æ•°æ®")
                            tracker.mark_success(ts_code, 0)
                            # æ— æ•°æ®ç®—æˆåŠŸï¼ˆå¯èƒ½åœç‰Œï¼‰
                            failure_monitor.on_success()
                            pbar.update(1)
                            continue

                        # å­˜å‚¨æ•°æ®
                        inserted = handler.insert_daily(
                            ts_code=ts_code,
                            data=data,
                            deduplicate=True
                        )

                        # æ ‡è®°æˆåŠŸ
                        tracker.mark_success(ts_code, inserted)
                        success_count += 1
                        # é€šçŸ¥ç›‘æ§å™¨é‡‡é›†æˆåŠŸ
                        failure_monitor.on_success()

                        logger.debug(f"âœ“ {ts_code}: {inserted} æ¡è®°å½•")

                    except Exception as e:
                        error_msg = str(e)

                        # åˆ¤æ–­æ˜¯å¦ä¸ºé™æµé”™è¯¯
                        if is_rate_limit_error(e):
                            logger.warning(f"ğŸš¨ {ts_code} è§¦å‘é™æµ: {error_msg}")

                            # æ ¹æ®å½“å‰çŠ¶æ€é€šçŸ¥é™æµæ¢æµ‹å™¨
                            if rate_limit_detector.state == "NORMAL":
                                await rate_limit_detector.on_rate_limit_triggered()
                            elif rate_limit_detector.state == "PROBING":
                                await rate_limit_detector.on_probe_failed()
                            elif rate_limit_detector.state == "CONFIRMED":
                                await rate_limit_detector.on_rate_limit_re_triggered()

                            # æ ‡è®°å¤±è´¥ï¼ˆåç»­å¯é‡è¯•ï¼‰
                            tracker.mark_failed(ts_code, f"é™æµ: {error_msg}")
                            failed_count += 1
                            # ä¸ç»§ç»­é‡‡é›†å½“å‰è‚¡ç¥¨ï¼Œç­‰å¾…æ¢æµ‹å®Œæˆ
                        else:
                            # å…¶ä»–é”™è¯¯
                            logger.error(f"âœ— {ts_code} é‡‡é›†å¤±è´¥: {error_msg}")
                            tracker.mark_failed(ts_code, error_msg)
                            failed_count += 1
                            # é€šçŸ¥ç›‘æ§å™¨é‡‡é›†å¤±è´¥
                            failure_monitor.on_failure(error_msg)

                    finally:
                        pbar.update(1)

        # ç”ŸæˆæŠ¥å‘Š
        logger.info("=" * 80)
        logger.info("å›å¡«å®Œæˆ")
        logger.info("=" * 80)

        # æ‰“å°ç›‘æ§ç»Ÿè®¡
        monitor_stats = failure_monitor.get_stats()
        if monitor_stats["pause_count"] > 0:
            logger.info(f"ç›‘æ§ç»Ÿè®¡: æš‚åœ{monitor_stats['pause_count']}æ¬¡, æ€»å¤±è´¥{monitor_stats['total_failures']}æ¬¡")

        # æ‰“å°é™æµæ¢æµ‹ç»Ÿè®¡
        rate_limit_detector.print_summary()

        report = _generate_report(tracker, start_time)
        _print_report(report)

        return report

    except Exception as e:
        logger.error(f"å›å¡«å¤±è´¥: {e}")
        raise

    finally:
        close_db_client()


async def retry_failed_stocks():
    """
    é‡è¯•å¤±è´¥çš„è‚¡ç¥¨

    ä»è¿›åº¦æ–‡ä»¶ä¸­è¯»å–å¤±è´¥è‚¡ç¥¨åˆ—è¡¨ï¼Œé‡æ–°é‡‡é›†

    Returns:
        é‡‡é›†æŠ¥å‘Šå­—å…¸
    """
    logger.info("=" * 80)
    logger.info("é‡è¯•å¤±è´¥è‚¡ç¥¨")
    logger.info("=" * 80)

    start_time = datetime.now()

    try:
        # åŠ è½½è¿›åº¦
        tracker = ProgressTracker()
        if not tracker.has_progress():
            logger.error("æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œæ— æ³•é‡è¯•")
            return None

        failed_stocks = tracker.get_failed_stocks()
        failed_details = tracker.get_failed_details()

        if not failed_stocks:
            logger.info("æ²¡æœ‰å¤±è´¥çš„è‚¡ç¥¨éœ€è¦é‡è¯•")
            return None

        logger.info(f"å‘ç° {len(failed_stocks)} åªå¤±è´¥è‚¡ç¥¨")

        # æ˜¾ç¤ºå¤±è´¥è¯¦æƒ…ï¼ˆå‰10ä¸ªï¼‰
        for ts_code in failed_stocks[:10]:
            error = failed_details.get(ts_code, "æœªçŸ¥é”™è¯¯")
            logger.warning(f"  - {ts_code}: {error}")
        if len(failed_stocks) > 10:
            logger.warning(f"  ... è¿˜æœ‰ {len(failed_stocks) - 10} åª")

        # è·å–æ—¥æœŸèŒƒå›´
        start_date = tracker.progress_data.get("start_date")
        end_date = tracker.progress_data.get("end_date")
        logger.info(f"æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")

        # åˆå§‹åŒ–é‡‡é›†å™¨å’Œå­˜å‚¨
        daily_collector = DailyCollector()
        client = get_db_client()
        handler = ClickHouseHandler(client)

        # åˆå§‹åŒ–å¤±è´¥ç›‘æ§å™¨
        failure_monitor = FailureMonitor(
            threshold=10,
            pause_duration=60,
            enable=True
        )

        # åˆå§‹åŒ–æ™ºèƒ½é™æµæ¢æµ‹å™¨ï¼ˆæŒ‡å®šæ•°æ®æºå’Œæ¥å£ï¼‰
        rate_limit_detector = RateLimitDetector(
            enable=True,
            source="akshare",
            interface="stock_zh_a_hist",
            data_type="daily",
            description="Aè‚¡æ—¥çº¿æ•°æ®"
        )
        logger.info("æ™ºèƒ½é™æµæ¢æµ‹: å·²å¯ç”¨")

        success_count = 0
        failed_count = 0

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(failed_stocks), desc="é‡è¯•è¿›åº¦", unit="åª", ncols=100) as pbar:
            for ts_code in failed_stocks:
                # æ£€æŸ¥å¤±è´¥ç›‘æ§å™¨æ˜¯å¦éœ€è¦æš‚åœ
                failure_monitor.wait_if_paused()

                # æ£€æŸ¥æ™ºèƒ½é™æµæ¢æµ‹å™¨æ˜¯å¦éœ€è¦æš‚åœ
                should_wait, wait_seconds = await rate_limit_detector.should_pause()
                if should_wait:
                    if rate_limit_detector.state == "PROBING":
                        pbar.set_postfix_str(f"æ¢æµ‹ç­‰å¾… (ç¬¬{rate_limit_detector.probe_count + 1}æ¬¡)")
                    elif rate_limit_detector.state == "CONFIRMED":
                        pbar.set_postfix_str(f"æ™ºèƒ½æš‚åœ ({wait_seconds}ç§’)")
                    await asyncio.sleep(wait_seconds)

                try:
                    pbar.set_postfix_str(f"å½“å‰: {ts_code}")

                    # é‡‡é›†æ•°æ®
                    data = await daily_collector.collect(
                        symbol=ts_code,
                        start_date=start_date,
                        end_date=end_date
                    )

                    # è®°å½•æˆåŠŸè¯·æ±‚
                    rate_limit_detector.record_success()

                    # å¦‚æœåœ¨æ¢æµ‹é˜¶æ®µè¯•æ¢æˆåŠŸï¼Œç¡®è®¤çª—å£
                    if rate_limit_detector.state == "PROBING":
                        await rate_limit_detector.on_probe_success()
                        logger.info("ç»§ç»­é‡è¯•...")

                    if not data:
                        logger.debug(f"{ts_code} æ— æ•°æ®")
                        tracker.mark_success(ts_code, 0)
                        failure_monitor.on_success()
                        success_count += 1
                        pbar.update(1)
                        continue

                    # å­˜å‚¨æ•°æ®
                    inserted = handler.insert_daily(
                        ts_code=ts_code,
                        data=data,
                        deduplicate=True
                    )

                    # æ ‡è®°æˆåŠŸ
                    tracker.mark_success(ts_code, inserted)
                    failure_monitor.on_success()
                    success_count += 1

                    logger.info(f"âœ“ {ts_code}: {inserted} æ¡è®°å½•")

                except Exception as e:
                    error_msg = str(e)

                    # åˆ¤æ–­æ˜¯å¦ä¸ºé™æµé”™è¯¯
                    if is_rate_limit_error(e):
                        logger.warning(f"ğŸš¨ {ts_code} è§¦å‘é™æµ: {error_msg}")

                        # æ ¹æ®å½“å‰çŠ¶æ€é€šçŸ¥é™æµæ¢æµ‹å™¨
                        if rate_limit_detector.state == "NORMAL":
                            await rate_limit_detector.on_rate_limit_triggered()
                        elif rate_limit_detector.state == "PROBING":
                            await rate_limit_detector.on_probe_failed()
                        elif rate_limit_detector.state == "CONFIRMED":
                            await rate_limit_detector.on_rate_limit_re_triggered()
                        tracker.mark_failed(ts_code, f"é™æµ: {error_msg}")
                        failed_count += 1
                    else:
                        logger.error(f"âœ— {ts_code} é‡è¯•å¤±è´¥: {error_msg}")
                        tracker.mark_failed(ts_code, error_msg)
                        failure_monitor.on_failure(error_msg)
                        failed_count += 1

                finally:
                    pbar.update(1)

        # ç”ŸæˆæŠ¥å‘Š
        logger.info("=" * 80)
        logger.info("é‡è¯•å®Œæˆ")
        logger.info("=" * 80)

        # æ‰“å°ç›‘æ§ç»Ÿè®¡
        monitor_stats = failure_monitor.get_stats()
        if monitor_stats["pause_count"] > 0:
            logger.info(f"ç›‘æ§ç»Ÿè®¡: æš‚åœ{monitor_stats['pause_count']}æ¬¡, æ€»å¤±è´¥{monitor_stats['total_failures']}æ¬¡")

        # æ‰“å°é™æµæ¢æµ‹ç»Ÿè®¡
        rate_limit_detector.print_summary()

        report = {
            "total_retried": len(failed_stocks),
            "success": success_count,
            "failed": failed_count,
            "duration": (datetime.now() - start_time).total_seconds()
        }

        logger.info(f"é‡è¯•è‚¡ç¥¨æ•°: {report['total_retried']}")
        logger.info(f"æˆåŠŸ: {report['success']}")
        logger.info(f"å¤±è´¥: {report['failed']}")
        logger.info(f"è€—æ—¶: {report['duration']:.1f} ç§’")

        # æ˜¾ç¤ºå‰©ä½™å¤±è´¥è‚¡ç¥¨
        remaining_failed = tracker.get_failed_stocks()
        if remaining_failed:
            logger.warning(f"ä»æœ‰ {len(remaining_failed)} åªå¤±è´¥: {', '.join(remaining_failed[:5])}")
            if len(remaining_failed) > 5:
                logger.warning(f"... è¿˜æœ‰ {len(remaining_failed) - 5} åª")

        return report

    except Exception as e:
        logger.error(f"é‡è¯•å¤±è´¥: {e}")
        raise

    finally:
        close_db_client()


async def backfill_specific_stocks(
    symbols: list[str],
    start_date: str,
    end_date: str
):
    """
    å›å¡«æŒ‡å®šè‚¡ç¥¨æ•°æ®

    Args:
        symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ

    Returns:
        é‡‡é›†æŠ¥å‘Šå­—å…¸
    """
    logger.info("=" * 80)
    logger.info("å›å¡«æŒ‡å®šè‚¡ç¥¨")
    logger.info("=" * 80)
    logger.info(f"è‚¡ç¥¨åˆ—è¡¨: {', '.join(symbols)}")
    logger.info(f"æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")

    start_time = datetime.now()

    try:
        # åˆå§‹åŒ–é‡‡é›†å™¨å’Œå­˜å‚¨
        daily_collector = DailyCollector()
        client = get_db_client()
        handler = ClickHouseHandler(client)

        success_count = 0
        failed_count = 0
        total_records = 0

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(symbols), desc="é‡‡é›†è¿›åº¦", unit="åª", ncols=100) as pbar:
            for ts_code in symbols:
                try:
                    pbar.set_postfix_str(f"å½“å‰: {ts_code}")

                    # é‡‡é›†æ•°æ®
                    data = await daily_collector.collect(
                        symbol=ts_code,
                        start_date=start_date,
                        end_date=end_date
                    )

                    if not data:
                        logger.warning(f"{ts_code} æ— æ•°æ®")
                        pbar.update(1)
                        continue

                    # å­˜å‚¨æ•°æ®
                    inserted = handler.insert_daily(
                        ts_code=ts_code,
                        data=data,
                        deduplicate=True
                    )

                    success_count += 1
                    total_records += inserted
                    logger.info(f"âœ“ {ts_code}: {inserted} æ¡è®°å½•")

                except Exception as e:
                    logger.error(f"âœ— {ts_code} é‡‡é›†å¤±è´¥: {e}")
                    failed_count += 1

                finally:
                    pbar.update(1)

        # ç”Ÿæˆç®€åŒ–æŠ¥å‘Š
        report = {
            "total_stocks": len(symbols),
            "success": success_count,
            "failed": failed_count,
            "total_records": total_records,
            "duration": (datetime.now() - start_time).total_seconds()
        }

        logger.info("=" * 80)
        logger.info("å›å¡«å®Œæˆ")
        logger.info("=" * 80)
        _print_report(report)

        return report

    except Exception as e:
        logger.error(f"å›å¡«å¤±è´¥: {e}")
        raise

    finally:
        close_db_client()


def _generate_report(tracker: ProgressTracker, start_time: datetime) -> dict:
    """
    ç”Ÿæˆé‡‡é›†æŠ¥å‘Š

    Args:
        tracker: è¿›åº¦è·Ÿè¸ªå™¨
        start_time: å¼€å§‹æ—¶é—´

    Returns:
        æŠ¥å‘Šå­—å…¸
    """
    stats = tracker.get_statistics()
    duration = (datetime.now() - start_time).total_seconds()

    return {
        "total_stocks": stats["total_stocks"],
        "completed": stats["completed"],
        "success": stats["success"],
        "failed": stats["failed"],
        "total_records": stats["total_records"],
        "duration": duration,
        "failed_stocks": tracker.get_failed_stocks()
    }


def _print_report(report: dict):
    """
    æ‰“å°é‡‡é›†æŠ¥å‘Š

    Args:
        report: æŠ¥å‘Šå­—å…¸
    """
    logger.info(f"æ€»è‚¡ç¥¨æ•°: {report.get('total_stocks', 0)}")
    logger.info(f"æˆåŠŸ: {report.get('success', 0)}")
    logger.info(f"å¤±è´¥: {report.get('failed', 0)}")
    logger.info(f"æ€»è®°å½•æ•°: {report.get('total_records', 0)}")
    logger.info(f"è€—æ—¶: {report.get('duration', 0):.1f} ç§’")

    if report.get("failed_stocks"):
        failed = report["failed_stocks"]
        logger.warning(f"å¤±è´¥è‚¡ç¥¨ ({len(failed)}): {', '.join(failed[:10])}")
        if len(failed) > 10:
            logger.warning(f"... è¿˜æœ‰ {len(failed) - 10} åª")


def main():
    """
    ä¸»å‡½æ•°
    """
    # åˆå§‹åŒ–æ—¥å¿—
    setup_logger()

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="A-Share Hub - å†å²æ•°æ®å›å¡«è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å…¨å¸‚åœºå›å¡«ï¼ˆæ¨èï¼‰
  python scripts/backfill.py --start-date 20200101 --all

  # æŒ‡å®šè‚¡ç¥¨å›å¡«
  python scripts/backfill.py --start-date 20200101 --symbols 000001.SZ,600000.SH

  # æ–­ç‚¹ç»­ä¼ 
  python scripts/backfill.py --resume

  # é‡è¯•å¤±è´¥è‚¡ç¥¨
  python scripts/backfill.py --retry-failed

  # æ§åˆ¶å¹¶å‘æ•°
  python scripts/backfill.py --start-date 20200101 --all --concurrency 3

  # æ¸…é™¤è¿›åº¦é‡æ–°å¼€å§‹
  python scripts/backfill.py --start-date 20200101 --all --clean
        """
    )

    parser.add_argument(
        "--start-date",
        type=str,
        help="å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼Œå¦‚ 20200101"
    )
    parser.add_argument(
        "--end-date",
        type=str,
        default=format_date(datetime.now()),
        help="ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼Œé»˜è®¤ä»Šå¤©"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="å›å¡«å…¨å¸‚åœºè‚¡ç¥¨"
    )
    parser.add_argument(
        "--symbols",
        type=str,
        help="æŒ‡å®šè‚¡ç¥¨ä»£ç ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œå¦‚ 000001.SZ,600000.SH"
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="æ–­ç‚¹ç»­ä¼ ï¼ˆä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­ï¼‰"
    )
    parser.add_argument(
        "--retry-failed",
        action="store_true",
        help="é‡è¯•å¤±è´¥çš„è‚¡ç¥¨"
    )
    parser.add_argument(
        "--concurrency",
        type=int,
        default=1,
        help="å¹¶å‘æ•°ï¼ˆé»˜è®¤1ï¼Œå»ºè®®ä¸è¶…è¿‡5ï¼‰"
    )
    parser.add_argument(
        "--clean",
        action="store_true",
        help="æ¸…é™¤è¿›åº¦é‡æ–°å¼€å§‹"
    )
    parser.add_argument(
        "--show-boundaries",
        action="store_true",
        help="æ˜¾ç¤ºé™æµè¾¹ç•Œä¿¡æ¯"
    )

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.show_boundaries:
        # æ˜¾ç¤ºé™æµè¾¹ç•Œä¿¡æ¯
        from utils.rate_limit_detector import print_all_boundaries
        print_all_boundaries()
        return

    if args.clean:
        tracker = ProgressTracker()
        tracker.clear_progress()
        logger.info("è¿›åº¦å·²æ¸…é™¤")
        return

    if args.retry_failed:
        # é‡è¯•å¤±è´¥è‚¡ç¥¨æ¨¡å¼
        asyncio.run(retry_failed_stocks())

    elif args.resume:
        # æ–­ç‚¹ç»­ä¼ æ¨¡å¼
        tracker = ProgressTracker()
        if not tracker.has_progress():
            logger.error("æœªæ‰¾åˆ°å¯æ¢å¤çš„è¿›åº¦ï¼Œè¯·å…ˆæ‰§è¡Œå›å¡«ä»»åŠ¡")
            sys.exit(1)

        # ä»è¿›åº¦ä¸­è·å–æ—¥æœŸèŒƒå›´
        start_date = tracker.progress_data.get("start_date")
        end_date = tracker.progress_data.get("end_date")

        asyncio.run(backfill_all_stocks(
            start_date=start_date,
            end_date=end_date,
            concurrency=args.concurrency,
            resume=True
        ))

    elif args.all:
        # å…¨å¸‚åœºå›å¡«
        if not args.start_date:
            logger.error("å…¨å¸‚åœºå›å¡«éœ€è¦æŒ‡å®š--start-dateå‚æ•°")
            sys.exit(1)

        asyncio.run(backfill_all_stocks(
            start_date=args.start_date,
            end_date=args.end_date,
            concurrency=args.concurrency,
            resume=False
        ))

    elif args.symbols:
        # æŒ‡å®šè‚¡ç¥¨å›å¡«
        if not args.start_date:
            logger.error("æŒ‡å®šè‚¡ç¥¨å›å¡«éœ€è¦æŒ‡å®š--start-dateå‚æ•°")
            sys.exit(1)

        symbols = [s.strip() for s in args.symbols.split(",")]
        asyncio.run(backfill_specific_stocks(
            symbols=symbols,
            start_date=args.start_date,
            end_date=args.end_date
        ))

    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
